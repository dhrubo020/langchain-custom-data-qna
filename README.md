# langchain-custom-data-qna

# LangChain Flow Overview

This project demonstrates a typical workflow using LangChain to build natural language applications with custom data.

---

## 📥 Input

- **User Input:** A natural language query  
- **System Output:** A natural language response generated by the LLM

---

## ⚙️ Setup Process

1. **Data Ingestion**
   - Load and clean your data.
   - Split it into smaller chunks and convert it into a list of documents.

2. **Embedding & Vector Store**
   - Generate embeddings from the documents.
   - Store the embeddings in a vector database (e.g., FAISS, Chroma, Pinecone).

3. **Prompt Template**
   - Define a template that guides how the query and context are presented to the language model.

4. **Context Retrieval**
   - Retrieve the most relevant documents from the vector store based on semantic similarity to the query.

5. **LLM Configuration**
   - Set up and configure your language model (e.g., OpenAI, Claude, Mistral).

6. **Chain Setup**
   - Combine the prompt template and LLM into a chain for processing input and generating output.

---

## 🚀 Execution

- On each user query:
  - The query is embedded and matched against the vector database.
  - Relevant context is retrieved.
  - The chain is invoked with the query and context.
  - The LLM generates a natural language response.

---

## ✅ Output

- A context-aware, accurate, and fluent natural language response is returned to the user.

---
## 🛠️ Installation & Running

```bash
pipenv install
pipenv run main.py
